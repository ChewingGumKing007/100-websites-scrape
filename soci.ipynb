{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1815826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.soci.org/events'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e919f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Feb  8 15:45:44 2022\n",
    "\n",
    "@author: ChewingGumKing_OJF\n",
    "\"\"\"\n",
    "\n",
    "#loads necessary libraries\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import os\n",
    "from datetime import datetime\n",
    "from datetime import date\n",
    "import openpyxl\n",
    "#import pyautogui\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "#import shutil\n",
    "#import glob\n",
    "import os\n",
    "import warnings\n",
    "#import itertools\n",
    "import json\n",
    "import pandas as pd\n",
    "import pyperclip as pc\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "from random import randint\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "#import sys\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f717179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r'C:\\Users\\840 g3\\Desktop\\chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68d843ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f8a0760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_more_xpath = 'events-showmorebtn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb0d0bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag=True\n",
    "while flag:\n",
    "    try:\n",
    "        soup = bs(driver.page_source,'lxml')\n",
    "        guag= soup.find('div',{'id':'events-grid'}) \n",
    "\n",
    "        time.sleep(2)\n",
    "        #scroll the skill into view\n",
    "        element=driver.find_element_by_xpath(f\"/html/body/div[2]/div[5]/div/section[2]/div/div/span\")\n",
    "        driver.execute_script(\"return arguments[0].scrollIntoView();\", element)\n",
    "        time.sleep(1)\n",
    "        driver.find_element_by_xpath('/html/body/div[2]/div[5]/div/section[2]/div/div/span').click()\n",
    "        #time.sleep(4)\n",
    "        soup = bs(driver.page_source,'lxml')\n",
    "        guar=soup.find('div',{'id':'events-grid'})\n",
    "        #time.sleep(2)\n",
    "        if len(guag)==len(guar):\n",
    "            flag=False\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "420552a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(driver.page_source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c98d8f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "section = soup.find('section',{'class':'panel-item-list grid-container'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b7fe179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = section.find('div',{'class':'cell'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a75b2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = cell.find_all('div',{'class':'cell'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "666f302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in cells:\n",
    "    try:\n",
    "#         date = i.find('p').text\n",
    "#         title = i.find('h4').text\n",
    "        web = i.find('a')['href']\n",
    "        link='https://www.soci.org'+web\n",
    "        data.append(link)\n",
    "    except:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89ae2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d140a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'no-trailer-for-small-up' #p\n",
    "topic = 'hero-content' #div g2 also second to the last p organiser\n",
    "location = 'location' #p\n",
    "cost = 'no-margin' #p\n",
    "agenda = 'itinerary' #div\n",
    "speakers = 'speakers' #div id\n",
    "venue_contact = 'columns small-12 medium-5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c2594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "ass\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n",
      "asde\n"
     ]
    }
   ],
   "source": [
    "sma=[]\n",
    "for j in data:\n",
    "    driver.get(j)\n",
    "    links=j\n",
    "    soup2 = bs(driver.page_source,'lxml')\n",
    "    try:\n",
    "        date__ = soup2.find('p',{'class':'no-trailer-for-small-up'}).text.replace('â€“','-')\n",
    "    except:\n",
    "        date__=''\n",
    "    #############\n",
    "    #_date=mad\n",
    "    #*****************DATE********************\n",
    "    if date__=='':\n",
    "        start_date=end_date=''\n",
    "    else:\n",
    "        mad=date__\n",
    "        try:\n",
    "#             ma=re.search('([A-Sa-z]+\\W+\\d{1,2}.+\\W+\\d{4}|\\d{1,2}.+\\W+\\d{4})',_date)#, maxsplit=0)\n",
    "#             mad=ma.group()\n",
    "            \n",
    "            if '-' in mad:\n",
    "                st=mad.split('-')[0].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "                en=mad.split('-')[1].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "\n",
    "            elif 'and'in mad:\n",
    "                st=mad.split('and')[0].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "                en=mad.split('and')[1].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "\n",
    "            elif '&' in mad:\n",
    "                st=mad.split('&')[0].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "                en=mad.split('&')[1].strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "\n",
    "            else:\n",
    "                st=mad.strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "                en=mad.strip().replace(',','').replace('nd','').replace('st','').replace('th','').replace('rd','').replace('ugu','ugust')\n",
    "            #print(date__,st,en)\n",
    "            ####################\n",
    "            if any(c.isalpha() for c in st)==False:\n",
    "            #      print('leg')\n",
    "                pa=re.search('[A-Sa-z]+\\W+(\\d{4})',en)#, maxsplit=0)\n",
    "                sapa=pa.group()\n",
    "                start=st+' '+sapa\n",
    "                end=en\n",
    "            #                        02        April      2022\n",
    "            elif re.search('\\d{1,2}\\s+[A-Sa-z]{3,9}\\s\\d{4}',st):\n",
    "             #   print('awujale')\n",
    "                start=st\n",
    "                end=en\n",
    "\n",
    "            #*                        April       02      2022\n",
    "            elif re.search('[A-Sa-z]{3,9}\\s\\d{1,2}\\s+\\d{4}',st):\n",
    "            #       print('ala')\n",
    "                pa=re.search('(\\d{4})',st).group()\n",
    "                sa=re.search('([A-Sa-z]{3,9})',st).group()\n",
    "                ta=re.search('(\\d{1,2})',st).group()\n",
    "                start=ta+' '+sa+' '+pa\n",
    "                #\n",
    "                ba=re.search('(\\d{4})',en).group()\n",
    "                ca=re.search('([A-Sa-z]{3,9})',en).group()\n",
    "                da=re.search('(\\d{1,2})',en).group()\n",
    "                end=da+' '+ca+' '+ba     \n",
    "\n",
    "            #                      02     2022\n",
    "            elif re.search('\\d{1,2}\\s+\\d{4}',en):\n",
    "            #      print('kum')\n",
    "                pa=re.search('(\\d{4})',en).group()#, maxsplit=0)\n",
    "                sa=re.search('([A-Sa-z]{3,9})',st).group()\n",
    "                ta=re.search('(\\d{1,2})',st).group()\n",
    "\n",
    "                start=ta+' '+sa+' '+pa\n",
    "                #\n",
    "                ba=re.search('(\\d{4})',en).group()\n",
    "                ca=re.search('([A-Sa-z]{3,9})',st).group()\n",
    "                da=re.search('(\\d{1,2})',en).group()\n",
    "                end=da+' '+ca+' '+ba\n",
    "\n",
    "            #                      02         April\n",
    "            elif re.search('\\d{1,2}\\s+[A-Sa-z]{3,9}',st):\n",
    "            #        print('is')\n",
    "                pa=re.search('(\\d{4})',en)#, maxsplit=0)\n",
    "                sapa=pa.group()\n",
    "                start=st+' '+sapa\n",
    "                end=en\n",
    "\n",
    "            #*                      April           02\n",
    "            elif re.search('[A-Sa-z]{3,9}\\s+\\d{1,2}',st):\n",
    "            #      print('bad')\n",
    "                pa=re.search('(\\d{4})',en).group()#, maxsplit=0)\n",
    "                sa=re.search('([A-Sa-z]{3,9})',st).group()\n",
    "                ta=re.search('(\\d{1,2})',st).group()\n",
    "\n",
    "                start=ta+' '+sa+' '+pa\n",
    "                #\n",
    "                ba=re.search('(\\d{4})',en).group()\n",
    "                ca=re.search('([A-Sa-z]{3,9})',en).group()\n",
    "                da=re.search('(\\d{1,2})',en).group()\n",
    "                end=da+' '+ca+' '+ba\n",
    "\n",
    "            else:\n",
    "            #     print('shik')\n",
    "                start=end=''\n",
    "            if start=='':\n",
    "                start_date=end_date=''\n",
    "            else:\n",
    "                pick=[start, end]\n",
    "                spl_dt_obj = [datetime.strptime(v, '%d %B %Y') for v in pick]\n",
    "                date_= [z.strftime('%Y-%m-%d') for z in spl_dt_obj]\n",
    "                start_date=date_[0]\n",
    "                end_date=date_[1]\n",
    "        except:\n",
    "            start_date=end_date=''\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    ###############\n",
    "    \n",
    "#     if '-' in _date:\n",
    "#         ash=_date.split('-')\n",
    "#         start_date=\n",
    "#     elif '&' in _date:\n",
    "#     else:\n",
    "        \n",
    "        \n",
    "    topic_location = soup2.find('div',{'class':topic})\n",
    "    title = topic_location.find('h1').text\n",
    "    \n",
    "    loc = topic_location.find_all('p')[-1].text\n",
    "    if 'Online' in loc or 'Webinar' in loc:\n",
    "        on_off='1'\n",
    "        loc=''\n",
    "    else:\n",
    "        on_off='0'\n",
    "        loc=loc\n",
    "    #'Prices start from Â£90.00\n",
    "    \n",
    "    ct_ = soup2.find('p',{'class':cost})#.text.strip()#prices start from <?\n",
    "    if 'Share' in str(ct_) or str(ct_)=='None':\n",
    "        ticketlist=''\n",
    "    else:\n",
    "        cost_=str(ct_).replace('<strong','</strong').split('</strong>')[-2]\n",
    "        if 'Â£' in cost_:\n",
    "            ala=cost_.split('Â£')\n",
    "            if float(ala[1])>0:\n",
    "                ty='paid'\n",
    "                am=ala[1]\n",
    "                cu='Â£'\n",
    "\n",
    "            else:\n",
    "                ty='free'\n",
    "                am=''\n",
    "                cu=''\n",
    "        elif '$' in cost_:\n",
    "            ala=cost_.split('$')\n",
    "            if float(ala[1])>0:\n",
    "                ty='paid'\n",
    "                am=ala[1]\n",
    "                cu='$'\n",
    "            else:\n",
    "                ty='free'\n",
    "                am=''\n",
    "                cu=''\n",
    "        elif 'â‚¬' in cost_:\n",
    "            ala=cost_.split('â‚¬')\n",
    "            if float(ala[1])>0:\n",
    "                ty='paid'\n",
    "                am=ala[1]\n",
    "                cu='â‚¬'\n",
    "            else:\n",
    "                ty='free'\n",
    "                am=''\n",
    "                cu=''\n",
    "        else:\n",
    "            ty=''\n",
    "            am=''\n",
    "            cu=''\n",
    "        if ty=='':\n",
    "            ticketlist=''\n",
    "        else:\n",
    "            ticketlist={'type': ty,\n",
    "                         'price':am,\n",
    "                         'currency': cu}\n",
    "       # print(ty,am,cu)\n",
    "    try:    \n",
    "        orm=topic_location.find_all('p')[-5].text\n",
    "        ormi=topic_location.find_all('p')[-3].text\n",
    "    except:\n",
    "        orm=''\n",
    "        ormi=''\n",
    "    if 'Organi' in orm:\n",
    "        try:\n",
    "            org_na=topic_location.find_all('p')[-3].text\n",
    "        except:\n",
    "            org_na=''\n",
    "        try:\n",
    "            org_w=topic_location.find_all('p')[-3].find('a')['href']\n",
    "            or_w='https://www.soci.org'+org_w\n",
    "        except:\n",
    "            or_w='https://www.soci.org/'#allley\n",
    "        \n",
    "        \n",
    "    elif 'Organi' in ormi:\n",
    "        try:\n",
    "            org_na=topic_location.find_all('p')[-2].text\n",
    "        except:\n",
    "            org_na=''\n",
    "        try:\n",
    "            org_w=topic_location.find_all('p')[-2].find('a')['href']\n",
    "            if 'http' in org_w:\n",
    "                or_w=org_w\n",
    "            else:\n",
    "                or_w='https://www.soci.org'+org_w\n",
    "        except:\n",
    "            or_w='https://www.soci.org/'\n",
    "        \n",
    "    else:\n",
    "        org_na=''\n",
    "        or_w=''\n",
    "    try:    \n",
    "        ev_i=soup2.find('div',{'class':'event-content-panel'})\n",
    "        ev=ev_i.find_all('p')\n",
    "        if 'img' in str(ev[0]):\n",
    "            even=ev[1].text\n",
    "        else:\n",
    "            even=ev[0].text\n",
    "        \n",
    "    except:\n",
    "        even=''\n",
    "    \n",
    "    try:\n",
    "        ag = soup2.find('div',{'class':agenda}).find('dl')##needs sorting\n",
    "        all_ag = ag.find_all()\n",
    "\n",
    "        agenda_ = []\n",
    "        temp = []\n",
    "        for l in all_ag:\n",
    "\n",
    "            if '<dt>' in str(l):\n",
    "                if len(temp) !=0:\n",
    "                    agenda_.append([temp[0].text,[a.get_text(separator=', ') for a in temp[1:]]])\n",
    "                    temp=[]\n",
    "                    temp.append(l)\n",
    "                else:\n",
    "                    temp.append(l)\n",
    "            else:\n",
    "                temp.append(l)\n",
    "    except:\n",
    "        agenda_=[]\n",
    "    #print(agenda_)\n",
    "    rack=[]\n",
    "    for idx,z in enumerate(agenda_):\n",
    "        st=z[0]\n",
    "        if idx+1!=len(agenda_):\n",
    "            et=agenda_[idx+1][0]\n",
    "        else:\n",
    "            et=''\n",
    "        desc=''\n",
    "        titl=z[1][0]\n",
    "        da=start_date\n",
    "        ori={'start_time':st,\n",
    "             'end_time':et,\n",
    "             'day':da,\n",
    "             'desc':desc,\n",
    "             'title':titl}\n",
    "        rack.append(ori)\n",
    "    \n",
    "    try:\n",
    "        spe = soup2.find('div',{'id':speakers}).find_all('div',{'class':'columns small-12 medium-9'})\n",
    "        speak=[s.text.strip().replace('\\n\\n',', ') for s in spe]\n",
    "    except:\n",
    "        speak =''\n",
    "    try:\n",
    "        v_contact = soup2.find('div',{'class':venue_contact})\n",
    "        v_contact_list = v_contact.find_all('p')\n",
    "        address =v_contact_list[1].text.replace('\\n',' ')\n",
    "        #phone = v_contact_list[-2].text\n",
    "        email = v_contact_list[-1].text.split(':').strip()\n",
    "    except:\n",
    "        address = ''\n",
    "        #phone = ''\n",
    "        email = 'conferences@soci.org'\n",
    "        \n",
    "    ###############COUNTYR,GPU#################\n",
    "    def country(locale):\n",
    "        try:\n",
    "            google_url_for_location=\"https://www.google.com/search?q=\"+locale+\"&oq=\"+locale+\"&num=1\"\n",
    "            time.sleep(randint(0,3))\n",
    "            driver.get(google_url_for_location)\n",
    "            time.sleep(4)\n",
    "            google_map_url=driver.find_element_by_id('lu_map').click()\n",
    "            time.sleep(1)\n",
    "            country=driver.find_element_by_class_name('x3AX1-LfntMc-header-title-VdSJob').text\n",
    "            return(country)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return(\"\")\n",
    "    \n",
    "    ####################################\n",
    "    def get_google_map_url(location):\n",
    "        try:\n",
    "            google_url_for_location=\"https://www.google.com/search?q=\"+location+\"&oq=\"+location+\"&num=1\"\n",
    "            time.sleep(randint(0,3))\n",
    "            driver.get(google_url_for_location)\n",
    "            time.sleep(4)\n",
    "            google_map_url=driver.find_element_by_id('lu_map').click()\n",
    "            time.sleep(1)\n",
    "            google_map_url=driver.current_url\n",
    "            return(google_map_url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return(\"\")\n",
    "    org_name=org_na\n",
    "    org_web=or_w\n",
    "    org_pro=''   \n",
    "    if ticketlist=='':\n",
    "        ticket_list=ticketlist\n",
    "    else:\n",
    "        ticket_list=json.dumps(ticketlist)\n",
    "    event_info=even\n",
    "    time_=''\n",
    "    logo=''\n",
    "    sponsors=''\n",
    "    if rack==[] or rack=='':\n",
    "        agendalist=''\n",
    "    else:\n",
    "        agendalist=json.dumps(rack)\n",
    "    type_=''\n",
    "    category=''\n",
    "    if email=='':\n",
    "        mail_=''\n",
    "    else:\n",
    "        mail_=json.dumps(email)\n",
    "    if speak=='':\n",
    "        Speakerlist=''\n",
    "    else:\n",
    "        Speakerlist=json.dumps(speak)\n",
    "    event_web=links\n",
    "    if address=='':\n",
    "        venue=''\n",
    "        city=''\n",
    "        country=''\n",
    "        googlePlaceUrl=''\n",
    "    else:\n",
    "        venue=address\n",
    "        city=address\n",
    "        country=country(city)\n",
    "        googlePlaceUrl=get_google_map_url(venue)\n",
    "    sma.append([links,title,start_date,end_date,time_,event_info,ticket_list,\n",
    "                 org_pro,org_name,org_web,logo,sponsors,agendalist,\n",
    "                 type_,category,city,country,venue,event_web,googlePlaceUrl,mail_,\n",
    "                 Speakerlist,on_off])\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137877f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e826f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sma_dict=sma\n",
    "\n",
    "\n",
    "sma_df= pd.DataFrame(columns=['scrappedUrl','eventname','startdate','enddate','timing','eventinfo','ticketlist','orgProfile','orgName','orgWeb','logo','sponsor','agendalist','type','category','city','country','venue','event_website','googlePlaceUrl','ContactMail','Speakerlist','online_event'],data=sma_dict)\n",
    "sma_df.to_csv(\"sma.tsv\", sep = '\\t',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784b0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87963905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup2 = bs(driver.page_source,'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27044f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = soup2.find('p',{'class':date}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f863410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_location = soup2.find('div',{'class':topic})\n",
    "# topic_ = topic_location.find('h1').text\n",
    "# loc = topic_location.find_all('p')[-1].text\n",
    "# topic_,loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af832361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost_ = soup2.find('p',{'class':cost}).text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e57e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag = soup2.find('div',{'class':agenda}).find('dl')\n",
    "# all_ag = ag.find_all()\n",
    "# all_ag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6af0230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agenda_ = []\n",
    "# temp = []\n",
    "# for l in all_ag:\n",
    "    \n",
    "#     if '<dt>' in str(l):\n",
    "#         if len(temp) !=0:\n",
    "#             agenda_.append([temp[0].text,[a.text for a in temp[1:]]])\n",
    "#             temp=[]\n",
    "#             temp.append(l)\n",
    "#         else:\n",
    "#             temp.append(l)\n",
    "#     else:\n",
    "#         temp.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fec11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agenda_{{}{}{}{}}\n",
    "# rack=[]\n",
    "# for idx,z in enumerate(agenda_):\n",
    "#     st=z[0]\n",
    "#     if idx+1!=len(agenda_):\n",
    "#         et=agenda[idx+1][0]\n",
    "#     else:\n",
    "#         et=''\n",
    "#     desc=''\n",
    "#     titl=z[1][0]\n",
    "#     da=start_date\n",
    "#     ori={'start_time':st,\n",
    "#          'end_time':et,\n",
    "#          'day':da,\n",
    "#          'desc':desc,\n",
    "#          'title'=titl}\n",
    "#     rack.append(ori)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speak = soup2.find('div',{'class':speakers}).find_all('p')[-1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53412271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_contact = soup2.find('div',{'class':venue_contact})\n",
    "# v_contact_list = v_contact.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf69e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# address =v_contact_list[1].text.replace('\\n',' ')\n",
    "# phone = v_contact_list[-2].text\n",
    "# email = v_contact_list[-1].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(columns=['date','title'],data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2b390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1bc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.to_json(orient='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ebfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76489e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
